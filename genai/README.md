# NicheExplorer GenAI Service

The GenAI container is the text-analysis brain of NicheExplorer.  It fetches raw RSS content, groups related papers/posts into topic **trends**, assigns each trend a relevance score, and generates 768-dimensional vector embeddings for future similarity search.  All heavy NLP work happens here so the Java API-server and React front-end remain lightweight.

---

## 1 Â· Responsibilities in one glance
* **Feed ingestion** â€“ downloads up-to-date items from ArXiv or Reddit feeds selected by the API-server.
* **Keyword-based topic discovery** â€“ scans titles & summaries for a curated ML/AI keyword list and buckets articles by the first hit.
* **Trend creation** â€“ converts every bucket that has â‰¥2 articles into a *trend* with autogenerated title, description, article count and relevance (formula `50 + 3Ã—count`, capped 100).
* **Vector embeddings** â€“ builds a normalised 768-D vector for each article using Google Gemini; falls back to Sentence-Transformers if the key is missing.
* **REST endpoints** â€“ exposes `/extract-trends`, `/classify`, and `/embed` so the Spring API-server can orchestrate the pipeline.

```
RSS Articles â†’ Preprocessing â†’ Semantic Analysis â†’ Topic Clustering â†’ Trend Ranking â†’ Vector Embeddings
```

### **Service Structure**
```
src/
â”œâ”€â”€ main.py                 # FastAPI application entry point
â”œâ”€â”€ routers/                # API endpoint handlers
â”‚   â”œâ”€â”€ trends.py          # Trend extraction endpoints
â”‚   â”œâ”€â”€ classification.py   # Query classification
â”‚   â””â”€â”€ embedding.py       # Vector embedding operations
â”œâ”€â”€ services/               # Core ML services
â”‚   â”œâ”€â”€ langchain_trends_service.py  # Main trend analysis engine
â”‚   â”œâ”€â”€ google_client.py            # Google Gemini API integration
â”‚   â””â”€â”€ embedding_service.py        # Vector embedding service
â”œâ”€â”€ models/                 # Data models and schemas
â”‚   â””â”€â”€ schemas.py         # Pydantic models for API
â”œâ”€â”€ config/                 # Configuration management
â”‚   â””â”€â”€ settings.py        # Environment and model settings
â””â”€â”€ download_model.py      # Pre-download ML models
```

## ðŸ”„ Complete ML Pipeline

> Note â€“ raw embeddings and the similarity endpoints are **not** called by the current UI; they are stored in PostgreSQL for upcoming "find similar" and visualisation work.

---

## 3 Â· Processing pipeline (plain language)
1. **Download** â€“ Fetch up to 50 feed entries, drop items with empty titles/very short bodies or obvious spam.
2. **Normalise** â€“ Lower-case and strip HTML so keyword matching is reliable.
3. **Bucket** â€“ Iterate over the curated keyword list (`neural`, `vision`, `transformer`, â€¦).  The first keyword found in the text becomes the bucket key; if none match, the first meaningful word in the title is used so nothing is lost.
4. **Filter** â€“ Discard buckets with fewer than 2 members to avoid noise.
5. **Label & score** â€“ Build human readable title/description and compute relevance.
6. **Embed** â€“ Generate and normalise a vector per article; store for similarity search.
7. **Respond** â€“ Return the six highest-relevance trends to the API-server which forwards them unchanged to the client.

---

## 4 Â· Configuration & environment
* `GOOGLE_API_KEY` â€“ optional; unlocks Gemini embeddings.  If missing the service silently switches to the local Sentence-Transformer model.
* Processing limits such as `MAX_ARTICLES_PER_REQUEST` and `MIN_TREND_FREQUENCY` are centralised in `config/settings.py` and can be tuned without code changes.

---

## 5 Â· Public endpoints (no payloads shown)
1. **POST `/extract-trends`** â€“ main entry: returns the trend list described above.
2. **POST `/classify`** â€“ heuristic label whether a query is "research" or "community"; used by the API-server to pick ArXiv vs Reddit.
3. **POST `/embed`** â€“ returns one vector for supplied text; handy for CLI experiments.

---

This document intentionally omits implementation code so it can be pasted anywhere without syntax-highlighting noise while still giving developers a clear picture of how the GenAI layer feeds the rest of NicheExplorer. 